---
title: Overview
sidebarTitle: Ollama
mode: wide
---

All LangChain integrations with [Ollama](https://ollama.com/).

Ollama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.

For a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).

{/* ## Installation and Setup

Follow [these instructions](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) to set up and run a local Ollama instance. Ollama should start as a background service automatically.

With the service started, run `ollama pull <name-of-model>` to download a [model](https://ollama.ai/library), e.g.:

```bash
ollama pull gpt-oss:20b
```

- This will download the default tagged version of the model. Typically, the default points to the latest, smallest sized-parameter model.
- To view all downloaded models, run `ollama list`

We're now ready to install the `langchain-ollama` partner package:

<CodeGroup>
    ```bash pip
    pip install langchain-ollama
    ```

    ```bash uv
    uv add langchain-ollama
    ```
</CodeGroup> */}

## Model interfaces

<Columns cols={2}>
    <Card title="ChatOllama" href="/oss/integrations/chat/ollama" cta="Get started" icon="message" arrow>
        Ollama chat models.
    </Card>
    <Card title="OllamaLLM" href="/oss/integrations/llms/ollama" cta="Get started" icon="i-cursor" arrow>
        (Legacy) Ollama text completion models.
    </Card>
    <Card title="OllamaEmbeddings" href="/oss/integrations/text_embedding/ollama" cta="Get started" icon="microsoft" arrow>
        Ollama embedding models.
    </Card>
</Columns>
