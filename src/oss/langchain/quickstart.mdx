---
title: Quickstart
---



This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.

## Build a basic agent

Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.

:::python
```python
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="anthropic:claude-sonnet-4-5",
    tools=[get_weather],
    system_prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```
:::

:::js
```ts
import { createAgent, tool } from "langchain";
import * as z from "zod";

const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);
```
:::

<Info>
    For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.
</Info>

## Build a real-world agent

Next, build a practical weather forecasting agent that demonstrates key production concepts:

1. **Detailed system prompts** for better agent behavior
2. **Create tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions
6. **Create and run the agent** create a fully functional agent

Let's walk through each step:

<Steps>
    <Step title="Define the system prompt">
        The system prompt defines your agentâ€™s role and behavior. Keep it specific and actionable:

        :::python
        ```python wrap
        SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

        You have access to two tools:

        - get_weather_for_location: use this to get the weather for a specific location
        - get_user_location: use this to get the user's location

        If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""
        ```
        :::

        :::js
        ```ts
        const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

        You have access to two tools:

        - get_weather_for_location: use this to get the weather for a specific location
        - get_user_location: use this to get the user's location

        If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;
        ```
        :::
    </Step>
    <Step title="Create tools">
        :::python
        [Tools](/oss/langchain/tools) let a model interact with external systems by calling functions you define.
        Tools can depend on [runtime context](/oss/langchain/runtime) and also interact with [agent memory](/oss/langchain/short-term-memory).

        Notice below how the `get_user_location` tool uses runtime context:

        ```python
        from dataclasses import dataclass
        from langchain.tools import tool, ToolRuntime

        @tool
        def get_weather_for_location(city: str) -> str:
            """Get weather for a given city."""
            return f"It's always sunny in {city}!"

        @dataclass
        class Context:
            """Custom runtime context schema."""
            user_id: str

        @tool
        def get_user_location(runtime: ToolRuntime[Context]) -> str:
            """Retrieve user information based on user ID."""
            user_id = runtime.context.user_id
            return "Florida" if user_id == "1" else "SF"
        ```

        <Tip>
            Tools should be well-documented: their name, description, and argument names become part of the model's prompt.
            LangChain's @[`@tool` decorator][@tool] adds metadata and enables runtime injection via the `ToolRuntime` parameter.
        </Tip>
        :::

        :::js
        [Tools](/oss/langchain/tools) are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:

        ```ts
        import { type Runtime } from "@langchain/langgraph";
        import { tool } from "langchain";
        import * as z from "zod";

        const getWeather = tool(
          (input) => `It's always sunny in ${input.city}!`,
          {
            name: "get_weather_for_location",
            description: "Get the weather for a given city",
            schema: z.object({
              city: z.string().describe("The city to get the weather for"),
            }),
          }
        );

        type AgentRuntime = Runtime<{ user_id: string }>;

        const getUserLocation = tool(
          (_, config: AgentRuntime) => {
            const { user_id } = config.context;
            return user_id === "1" ? "Florida" : "SF";
          },
          {
            name: "get_user_location",
            description: "Retrieve user information based on user ID",
          }
        );
        ```

        <Note>
            [Zod](https://zod.dev/) is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.

            Alternatively, you can define the `schema` property as a [JSON schema](https://json-schema.org/overview/what-is-jsonschema) object. Keep in mind that JSON schemas **won't** be validated at runtime.

            <Accordion title="Example: Using JSON schema for tool input">
                ```ts
                const getWeather = tool(
                  ({ city }) => `It's always sunny in ${city}!`,
                  {
                    name: "get_weather_for_location",
                    description: "Get the weather for a given city",
                    schema: {
                      type: "object",
                      properties: {
                        city: {
                          type: "string",
                          description: "The city to get the weather for"
                        }
                      },
                      required: ["city"]
                    },
                  }
                );
            ```
            </Accordion>
        </Note>
        :::
    </Step>
    <Step title="Configure your model">
        Set up your [language model](/oss/langchain/models) with the right [parameters](/oss/langchain/models#parameters) for your use case:

        :::python
        ```python
        from langchain.chat_models import init_chat_model

        model = init_chat_model(
            "anthropic:claude-sonnet-4-5",
            temperature=0.5,
            timeout=10,
            max_tokens=1000
        )
        ```
        :::

        :::js
        ```ts
        import { initChatModel } from "langchain";

        const model = await initChatModel(
          "anthropic:claude-sonnet-4-5",
          { temperature: 0.5, timeout: 10, maxTokens: 1000 }
        );
        ```
        :::
    </Step>
    <Step title="Define response format">
        :::python
        Optionally, define a structured response format if you need the agent responses to match
        a specific schema.

        ```python
        from dataclasses import dataclass

        # We use a dataclass here, but Pydantic models are also supported.
        @dataclass
        class ResponseFormat:
            """Response schema for the agent."""
            # A punny response (always required)
            punny_response: str
            # Any interesting information about the weather if available
            weather_conditions: str | None = None
        ```
        :::

        :::js
        Optionally, define a structured response format if you need the agent responses to match
        a specific schema.

        ```ts
        const responseFormat = z.object({
          punny_response: z.string(),
          weather_conditions: z.string().optional(),
        });
        ```
        :::
    </Step>
    <Step title="Add memory">
        Add [memory](/oss/langchain/short-term-memory) to your agent to maintain state across interactions. This allows
        the agent to remember previous conversations and context.

        :::python
        ```python
        from langgraph.checkpoint.memory import InMemorySaver

        checkpointer = InMemorySaver()
        ```
        :::

        :::js
        ```ts
        import { MemorySaver } from "@langchain/langgraph";

        const checkpointer = new MemorySaver();
        ```
        :::

        <Info>
            In production, use a persistent checkpointer that saves to a database.
            See [Add and manage memory](/oss/langgraph/add-memory#manage-short-term-memory) for more details.
        </Info>
    </Step>
    <Step title="Create and run the agent">
        Now assemble your agent with all the components and run it!

        :::python

        ```python
        agent = create_agent(
            model=model,
            system_prompt=SYSTEM_PROMPT,
            tools=[get_user_location, get_weather_for_location],
            context_schema=Context,
            response_format=ResponseFormat,
            checkpointer=checkpointer
        )

        # `thread_id` is a unique identifier for a given conversation.
        config = {"configurable": {"thread_id": "1"}}

        response = agent.invoke(
            {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
            config=config,
            context=Context(user_id="1")
        )

        print(response['structured_response'])
        # ResponseFormat(
        #     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
        #     weather_conditions="It's always sunny in Florida!"
        # )


        # Note that we can continue the conversation using the same `thread_id`.
        response = agent.invoke(
            {"messages": [{"role": "user", "content": "thank you!"}]},
            config=config,
            context=Context(user_id="1")
        )

        print(response['structured_response'])
        # ResponseFormat(
        #     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
        #     weather_conditions=None
        # )
        ```
        :::
        :::js
        ```ts
        import { createAgent } from "langchain";

        const agent = createAgent({
          model: "anthropic:claude-sonnet-4-5",
          prompt: systemPrompt,
          tools: [getUserLocation, getWeather],
          responseFormat,
          checkpointer,
        });

        // `thread_id` is a unique identifier for a given conversation.
        const config = {
          configurable: { thread_id: "1" },
          context: { user_id: "1" },
        };

        const response = await agent.invoke(
          { messages: [{ role: "user", content: "what is the weather outside?" }] },
          config
        );
        console.log(response.structuredResponse);
        // {
        //   punny_response: "Florida is still having a 'sun-derful' day ...",
        //   weather_conditions: "It's always sunny in Florida!"
        // }

        // Note that we can continue the conversation using the same `thread_id`.
        const thankYouResponse = await agent.invoke(
          { messages: [{ role: "user", content: "thank you!" }] },
          config
        );
        console.log(thankYouResponse.structuredResponse);
        // {
        //   punny_response: "You're 'thund-erfully' welcome! ...",
        //   weather_conditions: undefined
        // }
        ```
        :::
    </Step>
</Steps>

<Expandable title="Full example code">
:::python
```python
from dataclasses import dataclass

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.tools import tool, ToolRuntime
from langgraph.checkpoint.memory import InMemorySaver


# Define system prompt
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

# Define context schema
@dataclass
class Context:
    """Custom runtime context schema."""
    user_id: str

# Define tools
@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

@tool
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """Retrieve user information based on user ID."""
    user_id = runtime.context.user_id
    return "Florida" if user_id == "1" else "SF"

# Configure model
model = init_chat_model(
    "anthropic:claude-sonnet-4-5",
    temperature=0
)

# Define response format
@dataclass
class ResponseFormat:
    """Response schema for the agent."""
    # A punny response (always required)
    punny_response: str
    # Any interesting information about the weather if available
    weather_conditions: str | None = None

# Set up memory
checkpointer = InMemorySaver()

# Create agent
agent = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=[get_user_location, get_weather_for_location],
    context_schema=Context,
    response_format=ResponseFormat,
    checkpointer=checkpointer
)

# Run agent
# `thread_id` is a unique identifier for a given conversation.
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
#     weather_conditions="It's always sunny in Florida!"
# )


# Note that we can continue the conversation using the same `thread_id`.
response = agent.invoke(
    {"messages": [{"role": "user", "content": "thank you!"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
#     weather_conditions=None
# )
```
:::


:::js
```ts
import { createAgent, tool, initChatModel } from "langchain";
import { MemorySaver, type Runtime } from "@langchain/langgraph";
import * as z from "zod";

// Define system prompt
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;

// Define tools
const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const getUserLocation = tool(
  (_, config: Runtime<{ user_id: string}>) => {
    const { user_id } = config.context;
    return user_id === "1" ? "Florida" : "SF";
  },
  {
    name: "get_user_location",
    description: "Retrieve user information based on user ID",
    schema: z.object({}),
  }
);

// Configure model
const model = await initChatModel(
  "anthropic:claude-sonnet-4-5",
  { temperature: 0 }
);

// Define response format
const responseFormat = z.object({
  punny_response: z.string(),
  weather_conditions: z.string().optional(),
});

// Set up memory
const checkpointer = new MemorySaver();

// Create agent
const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5",
  prompt: systemPrompt,
  tools: [getUserLocation, getWeather],
  responseFormat,
  checkpointer,
});

// Run agent
// `thread_id` is a unique identifier for a given conversation.
const config = {
  configurable: { thread_id: "1" },
  context: { user_id: "1" },
};

const response = await agent.invoke(
  { messages: [{ role: "user", content: "what is the weather outside?" }] },
  config
);
console.log(response.structuredResponse);
// {
//   punny_response: "Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
//   weather_conditions: "It's always sunny in Florida!"
// }

// Note that we can continue the conversation using the same `thread_id`.
const thankYouResponse = await agent.invoke(
  { messages: [{ role: "user", content: "thank you!" }] },
  config
);
console.log(thankYouResponse.structuredResponse);
// {
//   punny_response: "You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
//   weather_conditions: undefined
// }
```
:::
</Expandable>

Congratulations! You now have an AI agent that can:

- **Understand context** and remember conversations
- **Use multiple tools** intelligently
- **Provide structured responses** in a consistent format
- **Handle user-specific information** through context
- **Maintain conversation state** across interactions
