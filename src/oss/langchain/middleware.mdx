---
title: Middleware
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

Middleware provides a way to more tightly control what happens inside the agent.

The core agent loop involves calling a `model`, letting it choose `tools` to execute, and then finishing when it calls no more tools.

![Core agent loop](/images/core_agent_loop.png)

Middleware provides control over what happens before and after those steps.

![Middleware flow diagram](/images/middleware_final.png)

## Overview

Middleware intercepts the agent execution flow at specific points, allowing you to:

- **Monitor** execution (logging, analytics, debugging)
- **Modify** requests and responses (prompts, tool selection, output formatting)
- **Control** flow (early termination, retries, fallbacks)
- **Enforce** policies (rate limits, guardrails, PII detection)

### Available Hooks

Build custom middleware by implementing any of these hooks on a subclass of the `AgentMiddleware` class:

:::python
| Hook | When it runs | Use cases |
|------|--------------|-----------|
| `before_agent` | Before calling the agent (once per invocation) | Load memory, validate input |
| `before_model` | Before each LLM call | Update prompts, trim messages |
| `wrap_model_call` | Around each LLM call | Retry logic, fallbacks, caching |
| `after_model` | After each LLM response | Validate output, apply guardrails |
| `wrap_tool_call` | Around each tool call | Retry, monitoring, modification |
| `after_agent` | After agent completes (once per invocation) | Save results, cleanup |
:::
:::js
| Hook | When it runs | Use cases |
|------|--------------|-----------|
| `beforeAgent` | Before calling the agent (once per invocation) | Load memory, validate input |
| `beforeModel` | Before each LLM call | Update prompts, trim messages |
| `wrapModelCall` | Around each LLM call | Retry logic, fallbacks, caching |
| `afterModel` | After each LLM response | Validate output, apply guardrails |
| `wrapToolCall` | Around each tool call | Retry, monitoring, modification |
| `afterAgent` | After agent completes (once per invocation) | Save results, cleanup |
:::

**Node-style hooks** (`before_agent`, `before_model`, `after_model`, `after_agent`) run sequentially and can:
- Return state updates to merge into agent state
- Return `{"jump_to": "end/model/tools"}` to jump to a different part of the graph
- Access and modify `state` and `runtime`

**Wrap-style hooks** (`wrap_model_call`, `wrap_tool_call`) intercept execution and can:
- Call the `handler` function zero, one, or multiple times
- Modify requests before calling the handler
- Transform responses before returning
- Implement retry logic, caching, or short-circuiting

### Using in an agent

:::python
You can use middleware in an agent by passing it to `create_agent`:

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[...],
    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],
)
```
:::

:::js
You can use middleware in an agent by passing it to `createAgent`:

```typescript
import {
  createAgent,
  summarizationMiddleware,
  humanInTheLoopMiddleware,
} from "langchain";

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [...],
  middleware: [summarizationMiddleware, humanInTheLoopMiddleware],
});
```
:::

## Built-in middleware

LangChain provides several built-in middleware to use off-the-shelf:

- [Summarization](#summarization) - Automatic conversation history summarization
- [Human-in-the-loop](#human-in-the-loop) - Human oversight for tool calls
- [Anthropic prompt caching](#anthropic-prompt-caching) - Enable native prompt caching
- [Planning](#planning) - Todo list management for complex tasks
- [Model fallback](#model-fallback) - Automatic fallback to alternative models
- [Tool selection](#tool-selection) - LLM-based tool filtering
- [Model call limit](#model-call-limit) - Enforce model call limits
- [Tool call limit](#tool-call-limit) - Enforce tool call limits
- [PII detection](#pii-detection) - Detect and handle personally identifiable information

### Summarization

The `SummarizationMiddleware` automatically manages conversation history by summarizing older messages when token limits are approached. This middleware monitors the total token count of messages and creates concise summaries to preserve context while staying within model limits.

**Key features:**

- Automatic token counting and threshold monitoring
- Intelligent message partitioning that preserves AI/Tool message pairs
- Customizable summary prompts and token limits

**Use Cases:**

- Long-running conversations that exceed token limits
- Multi-turn dialogues with extensive context

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="openai:gpt-4o-mini",
            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens
            messages_to_keep=20,  # Keep last 20 messages after summary
            summary_prompt="Custom prompt for summarization...",  # Optional
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "openai:gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
      summaryPrompt: "Custom prompt for summarization...", // Optional
    }),
  ],
});
```
:::

**Configuration options:**

:::python
- `model`: Language model to use for generating summaries (required)
- `max_tokens_before_summary`: Token threshold that triggers summarization
- `messages_to_keep`: Number of recent messages to preserve (default: 20)
- `token_counter`: Custom function for counting tokens (defaults to character-based approximation)
- `summary_prompt`: Custom prompt template for summary generation
- `summary_prefix`: Prefix added to system messages containing summaries (default: "## Previous conversation summary:")
:::

:::js

- `model`: Language model to use for generating summaries (required)
- `maxTokensBeforeSummary`: Token threshold that triggers summarization
- `messagesToKeep`: Number of recent messages to preserve (default: 20)
- `tokenCounter`: Custom function for counting tokens (defaults to character-based approximation)
- `summaryPrompt`: Custom prompt template for summary generation
- `summaryPrefix`: Prefix added to system messages containing summaries (default: "## Previous conversation summary:")
:::

The middleware ensures tool call integrity by:

1. Never splitting AI messages from their corresponding tool responses
2. Preserving the most recent messages for continuity
3. Including previous summaries in new summarization cycles

### Human-in-the-loop

The `HumanInTheLoopMiddleware` enables human oversight and intervention for tool calls made by the agents. Please
see [human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop) for more details.

This middleware intercepts tool executions and allows human operators to approve, modify, reject, or manually respond to tool calls before they execute.

### Anthropic prompt caching

`AnthropicPromptCachingMiddleware` is a middleware that enables you to enable Anthropic's native prompt caching.

Prompt caching enables optimal API usage by allowing resuming from specific prefixes in your prompts.
This is particularly useful for tasks with repetitive prompts or prompts with redundant information.

<Info>
Learn more about Anthropic Prompt Caching (strategies, limitations, etc.) [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).
</Info>

When using prompt caching, you'll likely want to use a checkpointer to store conversation
history across invocations.

:::python
```python
from langchain_anthropic import ChatAnthropic
from langchain.agents.middleware.prompt_caching import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

LONG_PROMPT = """
Please be a helpful assistant.

<Lots more context ...>
"""

agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-latest"),
    system_prompt=LONG_PROMPT,
    middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)

# cache store
agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

# cache hit, system prompt is cached
agent.invoke({"messages": [HumanMessage("What's my name?")]})
```
:::

:::js
```typescript
import { createAgent, HumanMessage, anthropicPromptCachingMiddleware } from "langchain";

const LONG_PROMPT = `
Please be a helpful assistant.

<Lots more context ...>
`;

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-latest",
  prompt: LONG_PROMPT,
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});

// cache store
await agent.invoke({
  messages: [new HumanMessage("Hi, my name is Bob")]
});

// cache hit, system prompt is cached
const result = await agent.invoke({
  messages: [new HumanMessage("What's my name?")]
});
```
:::

### Planning

The `PlanningMiddleware` provides todo list management capabilities to agents. This middleware adds a `write_todos` tool that allows agents to create and manage structured task lists for complex multi-step operations.

**Key features:**

- Automatic todo list tracking with status management
- Tool for agents to update progress
- Customizable prompts and tool descriptions

**Use cases:**

- Complex multi-step tasks requiring organization
- Tasks where progress visibility is important
- Agent workflows with multiple sequential operations

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import PlanningMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[...],
    middleware=[PlanningMiddleware()],
)

# Agent now has access to write_todos tool
result = await agent.invoke({"messages": [HumanMessage("Help me refactor my codebase")]})

# Access todos from result
print(result["todos"])  # [{"content": "...", "status": "completed", ...}, ...]
```

**Configuration options:**

- `system_prompt`: Custom guidance for using the todo tool (optional)
- `tool_description`: Custom description for the write_todos tool (optional)
:::

### Model fallback

The `ModelFallbackMiddleware` provides automatic fallback to alternative models on errors. When the primary model fails, it retries with fallback models in sequence until success or all models are exhausted.

**Use cases:**

- Improve reliability with backup models
- Cost optimization (try cheaper model first, fallback to more capable)
- Handle rate limits or service outages

:::python
```python
from langchain.agents.middleware import ModelFallbackMiddleware
from langchain.agents import create_agent

fallback = ModelFallbackMiddleware(
    "openai:gpt-4o-mini",  # Try first on error
    "anthropic:claude-3-5-sonnet-20241022",  # Then this
)

agent = create_agent(
    model="openai:gpt-4o",  # Primary model
    middleware=[fallback],
)

# If primary fails: tries gpt-4o-mini, then claude-3-5-sonnet
result = await agent.invoke({"messages": [HumanMessage("Hello")]})
```
:::

### Tool selection

The `LLMToolSelectorMiddleware` uses an LLM to dynamically select the most relevant tools before calling the main model. This is useful when you have many tools available but want to reduce token usage and help the model focus.

**Key features:**

- LLM-based tool relevance scoring
- Configurable maximum tool count
- Always-include tool lists
- Uses smaller/cheaper model for selection

**Use cases:**

- Agents with large tool catalogs
- Reducing token usage in model calls
- Improving tool selection accuracy

:::python
```python
from langchain.agents.middleware import LLMToolSelectorMiddleware
from langchain.agents import create_agent

middleware = LLMToolSelectorMiddleware(
    model="openai:gpt-4o-mini",  # Use cheaper model for selection
    max_tools=3,  # Limit to 3 most relevant tools
    always_include=["help_tool"],  # Always include this tool
)

agent = create_agent(
    model="openai:gpt-4o",
    tools=[tool1, tool2, tool3, tool4, tool5],  # Many tools
    middleware=[middleware],
)
```

**Configuration options:**

- `model`: Model to use for selection (defaults to agent's model)
- `system_prompt`: Instructions for the selection model
- `max_tools`: Maximum number of tools to select (default: no limit)
- `always_include`: Tool names to always include
:::

### Model call limit

The `ModelCallLimitMiddleware` tracks model call counts and enforces limits. It supports both thread-level (across multiple invocations) and run-level (per invocation) counting.

**Use cases:**

- Prevent infinite loops or runaway costs
- Enforce usage quotas
- Debug and monitor model call frequency

:::python
```python
from langchain.agents.middleware import ModelCallLimitMiddleware
from langchain.agents import create_agent

call_tracker = ModelCallLimitMiddleware(
    thread_limit=10,  # Max 10 calls per thread (persisted)
    run_limit=5,  # Max 5 calls per run (single invocation)
    exit_behavior="end",  # Jump to end when limit exceeded
)

agent = create_agent(
    model="openai:gpt-4o",
    middleware=[call_tracker],
    tools=[...],
)
```

**Configuration options:**

- `thread_limit`: Maximum calls per thread (persisted across runs)
- `run_limit`: Maximum calls per single invocation
- `exit_behavior`: `"end"` (graceful termination) or `"error"` (raise exception)
:::

### Tool call limit

The `ToolCallLimitMiddleware` tracks tool call counts and enforces limits, similar to model call limits.

:::python
```python
from langchain.agents.middleware import ToolCallLimitMiddleware

tool_limiter = ToolCallLimitMiddleware(
    thread_limit=20,
    run_limit=10,
    exit_behavior="end",
)
```

Configuration options are identical to `ModelCallLimitMiddleware`.
:::

### PII detection

The `PIIMiddleware` detects and handles Personally Identifiable Information (PII) in agent conversations. It can detect emails, credit cards, IP addresses, MAC addresses, and URLs in both user input and agent output.

**Built-in PII types:**

- `email`: Email addresses
- `credit_card`: Credit card numbers (validated with Luhn algorithm)
- `ip`: IP addresses (validated with stdlib)
- `mac_address`: MAC addresses
- `url`: URLs (both http/https and bare URLs)

**Strategies:**

- `block`: Raise an exception when PII is detected
- `redact`: Replace PII with `[REDACTED_TYPE]` placeholders
- `mask`: Partially mask PII (e.g., `****-****-****-1234` for credit card)
- `hash`: Replace PII with deterministic hash (e.g., `<email_hash:a1b2c3d4>`)

:::python
```python
from langchain.agents.middleware import PIIMiddleware
from langchain.agents import create_agent

# Redact all emails in user input
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[
        PIIMiddleware("email", strategy="redact"),
    ],
    tools=[...],
)

# Use different strategies for different PII types
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[
        PIIMiddleware("credit_card", strategy="mask"),
        PIIMiddleware("url", strategy="redact"),
        PIIMiddleware("ip", strategy="hash"),
    ],
    tools=[...],
)

# Custom PII type with regex
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[
        PIIMiddleware("api_key", detector=r"sk-[a-zA-Z0-9]{32}", strategy="block"),
    ],
    tools=[...],
)
```

**Configuration options:**

- `pii_type`: Type of PII to detect (built-in or custom name)
- `strategy`: How to handle detected PII
- `detector`: Custom detector function or regex pattern
- `apply_to_input`: Check user messages (default: `True`)
- `apply_to_output`: Check AI messages (default: `False`)
- `apply_to_tool_results`: Check tool results (default: `False`)
:::

## Custom middleware

### Creating middleware with hooks

Middleware for agents are subclasses of `AgentMiddleware`, which implement one or more of its hooks.

There are two styles of hooks:

1. **Node-style hooks** - Run sequentially at specific points
2. **Wrap-style hooks** - Intercept and control execution via handler callbacks

#### Node-style hooks

Node-style hooks run sequentially and can return state updates or trigger jumps:

- `before_agent`: runs before the agent starts
- `before_model`: runs before each model call
- `after_model`: runs after each model response
- `after_agent`: runs after the agent completes

**Example: Logging middleware**

:::python
```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langgraph.runtime import Runtime
from typing import Any

class LoggingMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"About to call model with {len(state['messages'])} messages")
        return None

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None
```
:::

:::js
```typescript
import { createMiddleware } from "langchain";

const loggingMiddleware = createMiddleware({
  name: "LoggingMiddleware",
  beforeModel: (state) => {
    console.log(`About to call model with ${state.messages.length} messages`);
    return;
  },
  afterModel: (state) => {
    const lastMessage = state.messages[state.messages.length - 1];
    console.log(`Model returned: ${lastMessage.content}`);
    return;
  },
});
```
:::

**Example: Conversation length limit**

:::python
```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain_core.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class MessageLimitMiddleware(AgentMiddleware):
    def __init__(self, max_messages: int = 50):
        super().__init__()
        self.max_messages = max_messages

    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        if len(state["messages"]) == self.max_messages:
            return {
                "messages": [AIMessage("Conversation limit reached.")],
                "jump_to": "end"
            }
        return None
```
:::

:::js
```typescript
import { createMiddleware, AIMessage } from "langchain";

const createMessageLimitMiddleware = (maxMessages: number = 50) => {
  return createMiddleware({
    name: "MessageLimitMiddleware",
    beforeModel: (state) => {
      if (state.messages.length === maxMessages) {
        return {
          messages: [new AIMessage("Conversation limit reached.")],
          jumpTo: "end",
        };
      }
      return;
    },
  });
};
```
:::

#### Wrap-style hooks (Interceptors)

Wrap-style hooks intercept execution and provide a handler callback. You control when (and if) the handler is called:

- `wrap_model_call`: wraps each model call
- `wrap_tool_call`: wraps each tool call

The handler can be called:
- **Zero times** - Short-circuit with cached/pre-computed result
- **Once** - Normal execution with request/response modification
- **Multiple times** - Retry logic or iterative improvement

**Example: Model retry middleware**

:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class RetryMiddleware(AgentMiddleware):
    def __init__(self, max_retries: int = 3):
        super().__init__()
        self.max_retries = max_retries

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        for attempt in range(self.max_retries):
            try:
                return handler(request)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")
```
:::

:::js
```typescript
import { createMiddleware } from "langchain";

const createRetryMiddleware = (maxRetries: number = 3) => {
  return createMiddleware({
    name: "RetryMiddleware",
    wrapModelCall: (request, handler) => {
      for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
          return handler(request);
        } catch (e) {
          if (attempt === maxRetries - 1) {
            throw e;
          }
          console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);
        }
      }
      throw new Error("Unreachable");
    },
  });
};
```
:::

**Example: Dynamic model selection**

:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

class DynamicModelMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Use different model based on conversation length
        if len(request.messages) > 10:
            request.model = init_chat_model("openai:gpt-4o")
        else:
            request.model = init_chat_model("openai:gpt-4o-mini")

        return handler(request)
```
:::

:::js
```typescript
import { createMiddleware, initChatModel } from "langchain";

const dynamicModelMiddleware = createMiddleware({
  name: "DynamicModelMiddleware",
  wrapModelCall: (request, handler) => {
    // Use different model based on conversation length
    const modifiedRequest = { ...request };
    if (request.messages.length > 10) {
      modifiedRequest.model = initChatModel("openai:gpt-4o");
    } else {
      modifiedRequest.model = initChatModel("openai:gpt-4o-mini");
    }
    return handler(modifiedRequest);
  },
});
```
:::

**Example: Tool call monitoring**

:::python
```python
from langchain.tools.tool_node import ToolCallRequest
from langchain.agents.middleware import AgentMiddleware
from langchain_core.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolMonitoringMiddleware(AgentMiddleware):
    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        print(f"Executing tool: {request.tool_call['name']}")
        print(f"Arguments: {request.tool_call['args']}")

        try:
            result = handler(request)
            print(f"Tool completed successfully")
            return result
        except Exception as e:
            print(f"Tool failed: {e}")
            raise
```
:::

:::js
```typescript
import { createMiddleware } from "langchain";

const toolMonitoringMiddleware = createMiddleware({
  name: "ToolMonitoringMiddleware",
  wrapToolCall: (request, handler) => {
    console.log(`Executing tool: ${request.toolCall.name}`);
    console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);

    try {
      const result = handler(request);
      console.log("Tool completed successfully");
      return result;
    } catch (e) {
      console.log(`Tool failed: ${e}`);
      throw e;
    }
  },
});
```
:::

### Custom state schema

Middleware can extend the agent's state with custom properties. Define a custom state type and set it as the `state_schema`:

:::python
```python
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
    state_schema = CustomState

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Access custom state properties
        count = state.get("model_call_count", 0)

        if count > 10:
            return {"jump_to": "end"}

        return None

    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Update custom state
        return {"model_call_count": state.get("model_call_count", 0) + 1}
```
:::

:::js
```typescript
import { createMiddleware, createAgent, HumanMessage } from "langchain";
import { z } from "zod";

// Middleware with custom state requirements
const callCounterMiddleware = createMiddleware({
  name: "CallCounterMiddleware",
  stateSchema: z.object({
    modelCallCount: z.number().default(0),
    userId: z.string().optional(),
  }),
  beforeModel: (state) => {
    // Access custom state properties
    if (state.modelCallCount > 10) {
      return { jumpTo: "end" };
    }
    return;
  },
  afterModel: (state) => {
    // Update custom state
    return { modelCallCount: state.modelCallCount + 1 };
  },
});
```
:::

When invoking the agent with custom state, TypeScript/type checking will enforce that required properties are provided:

:::python
```python
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[CallCounterMiddleware()],
    tools=[...],
)

# Invoke with custom state
result = agent.invoke({
    "messages": [HumanMessage("Hello")],
    "model_call_count": 0,
    "user_id": "user-123",
})
```
:::

:::js
```typescript
const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [...],
  middleware: [callCounterMiddleware] as const,
});

// TypeScript enforces required state properties
const result = await agent.invoke({
  messages: [new HumanMessage("Hello")],
  modelCallCount: 0, // Optional due to default value
  userId: "user-123", // Optional
});
```
:::

### Decorator-based middleware

For simple middleware that only needs a single hook, use decorator shortcuts:

:::python
```python
from langchain.agents.middleware import before_model, after_model, wrap_model_call
from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse
from langgraph.runtime import Runtime
from typing import Any, Callable

# Node-style decorator
@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

# Wrap-style decorator
@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception:
            if attempt == 2:
                raise
    return handler(request)  # This line is unreachable but satisfies type checker

# Use decorators in agent
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[log_before_model, retry_model],
    tools=[...],
)
```
:::

:::js
```typescript
import { createMiddleware, createAgent } from "langchain";

// Simple function-based middleware
const logBeforeModel = createMiddleware({
  name: "LogBeforeModel",
  beforeModel: (state) => {
    console.log(`About to call model with ${state.messages.length} messages`);
    return;
  },
});

// Wrap-style middleware
const retryModel = createMiddleware({
  name: "RetryModel",
  wrapModelCall: (request, handler) => {
    for (let attempt = 0; attempt < 3; attempt++) {
      try {
        return handler(request);
      } catch (e) {
        if (attempt === 2) throw e;
      }
    }
    throw new Error("Unreachable");
  },
});

// Use in agent
const agent = createAgent({
  model: "openai:gpt-4o",
  middleware: [logBeforeModel, retryModel],
  tools: [...],
});
```
:::

Available decorators:
- `@before_agent`
- `@before_model`
- `@after_model`
- `@after_agent`
- `@wrap_model_call`
- `@wrap_tool_call`
- `@dynamic_prompt` - Convenience for dynamic system prompts
:::

### Combining and ordering middleware

When using multiple middleware, they execute in a specific order:

**Node-style hooks:**
- `before_agent`: Runs in order (first to last)
- `before_model`: Runs in order (first to last)
- `after_model`: Runs in **reverse** order (last to first)
- `after_agent`: Runs in **reverse** order (last to first)

**Wrap-style hooks:**
- `wrap_model_call`: Composes like nested function calls (first middleware wraps all subsequent ones)
- `wrap_tool_call`: Composes like nested function calls

**Example:**

:::python
```python
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[middleware1, middleware2, middleware3],
    tools=[...],
)
```
:::

:::js
```typescript
const agent = createAgent({
  model: "openai:gpt-4o",
  middleware: [middleware1, middleware2, middleware3],
  tools: [...],
});
```
:::

Execution order:
1. `middleware1.before_agent()`
2. `middleware2.before_agent()`
3. `middleware3.before_agent()`
4. **Agent loop starts**
5. `middleware1.before_model()`
6. `middleware2.before_model()`
7. `middleware3.before_model()`
8. `middleware1.wrap_model_call()` → calls `middleware2.wrap_model_call()` → calls `middleware3.wrap_model_call()` → actual model call
9. `middleware3.after_model()`
10. `middleware2.after_model()`
11. `middleware1.after_model()`
12. **Agent loop ends**
13. `middleware3.after_agent()`
14. `middleware2.after_agent()`
15. `middleware1.after_agent()`

### Agent jumps

To exit early from middleware, return a dictionary with `jump_to`:

:::python
```python
class EarlyExitMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        # Check some condition
        if should_exit(state):
            return {
                "messages": [AIMessage("Exiting early due to condition.")],
                "jump_to": "end"
            }
        return None
```
:::

:::js
```typescript
import { createMiddleware, AIMessage } from "langchain";

const earlyExitMiddleware = createMiddleware({
  name: "EarlyExitMiddleware",
  beforeModel: (state) => {
    // Check some condition
    if (shouldExit(state)) {
      return {
        messages: [new AIMessage("Exiting early due to condition.")],
        jumpTo: "end",
      };
    }
    return;
  },
});
```
:::

Available jump targets:

- `"end"`: Jump to the end of the agent execution
- `"tools"`: Jump to the tools node
- `"model"`: Jump to the model node (or the first `before_model` hook)

**Important:** When jumping from `before_model` or `after_model`, jumping to `"model"` will cause all `before_model` middleware to run again.

To enable jumping, decorate your hook with `@hook_config(can_jump_to=[...])`:

:::python
```python
from langchain.agents.middleware import AgentMiddleware, hook_config
from typing import Any

class ConditionalMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end", "tools"])
    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        if some_condition(state):
            return {"jump_to": "end"}
        return None
```
:::

:::js
```typescript
import { createMiddleware } from "langchain";

const conditionalMiddleware = createMiddleware({
  name: "ConditionalMiddleware",
  afterModel: (state) => {
    if (someCondition(state)) {
      return { jumpTo: "end" };
    }
    return;
  },
});
```
:::

### Best practices

1. **Keep middleware focused** - Each middleware should do one thing well
2. **Handle errors gracefully** - Don't let middleware errors crash the agent
3. **Use appropriate hook types**:
   - Node-style for sequential logic (logging, validation)
   - Wrap-style for control flow (retry, fallback, caching)
4. **Document state requirements** - Clearly document any custom state properties
5. **Test middleware independently** - Unit test middleware before integrating
6. **Consider execution order** - Place critical middleware first in the list
7. **Use built-in middleware when possible** - Don't reinvent the wheel
:::

## Examples

### Dynamically selecting tools

In many applications, you may have a large set of tools, but only a small subset is relevant for a specific request. To optimize performance and accuracy, it's best to **expose only the tools that are needed for each request**.

Doing so provides several benefits:

* **Shorter prompts** – reducing unnecessary complexity.
* **Improved accuracy** – the model chooses from fewer options.
* **Permission control** – can select tools based on user permissions.

Use middleware to dynamically select which tools are available at runtime based on context.

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest
from typing import Callable

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Middleware to select relevant tools based on state/context."""
        # Select a small, relevant subset of tools based on state/context
        relevant_tools = select_relevant_tools(request.state, request.runtime)
        request.tools = relevant_tools
        return handler(request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    # Middleware can be used to select a smaller subset that's relevant for the given run.
    middleware=[ToolSelectorMiddleware()],
)
```
:::

:::js
```typescript
import { createAgent, createMiddleware } from "langchain";

const toolSelectorMiddleware = createMiddleware({
  name: "ToolSelector",
  wrapModelCall: (request, handler) => {
    // Select a small, relevant subset of tools based on state/context
    const relevantTools = selectRelevantTools(request.state, request.runtime);
    const modifiedRequest = { ...request, tools: relevantTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: allTools, // All available tools need to be registered upfront
  // Middleware can be used to select a smaller subset that's relevant for the given run.
  middleware: [toolSelectorMiddleware],
});
```
:::

<Expandable title="Extended example: Select tools based on runtime context">

This example shows how to select between GitHub and GitLab tools based on the user's provider.

:::python
```python
from dataclasses import dataclass
from typing import Literal, Callable

from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain_core.tools import tool

@tool
def github_create_issue(repo: str, title: str) -> dict:
    """Create an issue in a GitHub repository."""
    return {"url": f"https://github.com/{repo}/issues/1", "title": title}

@tool
def gitlab_create_issue(project: str, title: str) -> dict:
    """Create an issue in a GitLab project."""
    return {"url": f"https://gitlab.com/{project}/-/issues/1", "title": title}

all_tools = [github_create_issue, gitlab_create_issue]

@dataclass
class Context:
    provider: Literal["github", "gitlab"]

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Select tools based on the VCS provider."""
        provider = request.runtime.context.provider

        if provider == "gitlab":
            selected_tools = [t for t in request.tools if t.name == "gitlab_create_issue"]
        else:
            selected_tools = [t for t in request.tools if t.name == "github_create_issue"]

        request.tools = selected_tools
        return handler(request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,
    middleware=[ToolSelectorMiddleware()],
    context_schema=Context,
)

# Invoke with GitHub context
agent.invoke(
    {
        "messages": [{"role": "user", "content": "Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"}]
    },
    context=Context(provider="github"),
)
```
:::

:::js
```typescript
import { z } from "zod";
import { createAgent, createMiddleware, tool, HumanMessage } from "langchain";

const githubCreateIssue = tool(
  async ({ repo, title }) => ({
    url: `https://github.com/${repo}/issues/1`,
    title,
  }),
  {
    name: "github_create_issue",
    description: "Create an issue in a GitHub repository",
    schema: z.object({ repo: z.string(), title: z.string() }),
  }
);

const gitlabCreateIssue = tool(
  async ({ project, title }) => ({
    url: `https://gitlab.com/${project}/-/issues/1`,
    title,
  }),
  {
    name: "gitlab_create_issue",
    description: "Create an issue in a GitLab project",
    schema: z.object({ project: z.string(), title: z.string() }),
  }
);

const allTools = [githubCreateIssue, gitlabCreateIssue];

const toolSelector = createMiddleware({
  name: "toolSelector",
  contextSchema: z.object({ provider: z.enum(["github", "gitlab"]) }),
  wrapModelCall: (request, handler) => {
    const provider = request.runtime.context.provider;
    const toolName = provider === "gitlab" ? "gitlab_create_issue" : "github_create_issue";
    const selectedTools = request.tools.filter((t) => t.name === toolName);
    const modifiedRequest = { ...request, tools: selectedTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: allTools,
  middleware: [toolSelector],
});

// Invoke with GitHub context
await agent.invoke(
  {
    messages: [
      new HumanMessage("Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"),
    ],
  },
  {
    context: { provider: "github" },
  }
);
```
:::

**Key points:**

- Register all tools with the agent upfront
- Use middleware to select the relevant subset per request
- Define required context properties using `context_schema`
- Use context for configuration that doesn't change during execution
- Use state for values that change during the agent run
:::

</Expandable>

### Implementing rate limiting

Use middleware to implement rate limiting across model or tool calls:

:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable
import time

class RateLimitMiddleware(AgentMiddleware):
    def __init__(self, calls_per_minute: int = 10):
        super().__init__()
        self.calls_per_minute = calls_per_minute
        self.call_times = []

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Remove calls older than 1 minute
        now = time.time()
        self.call_times = [t for t in self.call_times if now - t < 60]

        # Check if we've exceeded the rate limit
        if len(self.call_times) >= self.calls_per_minute:
            oldest_call = self.call_times[0]
            wait_time = 60 - (now - oldest_call)
            print(f"Rate limit reached. Waiting {wait_time:.1f}s...")
            time.sleep(wait_time)
            self.call_times = []

        # Record this call
        self.call_times.append(time.time())

        return handler(request)
```
:::

### Implementing caching

Use `wrap_model_call` to implement response caching:

:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable
import hashlib
import json

class CachingMiddleware(AgentMiddleware):
    def __init__(self):
        super().__init__()
        self.cache = {}

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Create cache key from messages
        cache_key = hashlib.md5(
            json.dumps([m.dict() for m in request.messages]).encode()
        ).hexdigest()

        # Check cache
        if cache_key in self.cache:
            print("Cache hit!")
            return self.cache[cache_key]

        # Call model and cache result
        response = handler(request)
        self.cache[cache_key] = response

        return response
```
:::

### Validating tool outputs

Use `wrap_tool_call` to validate tool outputs and retry on failure:

:::python
```python
from langchain.agents.middleware import AgentMiddleware
from langchain.tools.tool_node import ToolCallRequest
from langchain_core.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolValidationMiddleware(AgentMiddleware):
    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        max_retries = 3

        for attempt in range(max_retries):
            result = handler(request)

            # Validate result
            if isinstance(result, ToolMessage) and self.is_valid(result):
                return result

            if attempt < max_retries - 1:
                print(f"Invalid tool result, retrying ({attempt + 1}/{max_retries})...")
                continue

            # Return error message after max retries
            return ToolMessage(
                content="Tool validation failed after multiple attempts",
                tool_call_id=request.tool_call["id"],
                status="error",
            )

        return result  # Unreachable but satisfies type checker

    def is_valid(self, result: ToolMessage) -> bool:
        # Implement your validation logic here
        return result.status != "error" and len(result.content) > 0
```
:::

### Content moderation

Implement content moderation using `after_model`:

:::python
```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain_core.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class ModerationMiddleware(AgentMiddleware):
    def __init__(self, blocked_words: list[str]):
        super().__init__()
        self.blocked_words = [w.lower() for w in blocked_words]

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        last_message = state["messages"][-1]

        if not isinstance(last_message, AIMessage):
            return None

        content = last_message.content.lower()

        # Check for blocked words
        for word in self.blocked_words:
            if word in content:
                return {
                    "messages": [AIMessage("I apologize, I cannot provide that response.")],
                    "jump_to": "end"
                }

        return None
```
:::

### Multi-model agent

Use multiple models for different parts of the conversation:

:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

class MultiModelMiddleware(AgentMiddleware):
    def __init__(self):
        super().__init__()
        self.fast_model = init_chat_model("openai:gpt-4o-mini")
        self.smart_model = init_chat_model("openai:gpt-4o")

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Use fast model for simple queries, smart model for complex ones
        last_message = request.messages[-1].content

        if self.is_complex_query(last_message):
            request.model = self.smart_model
        else:
            request.model = self.fast_model

        return handler(request)

    def is_complex_query(self, query: str) -> bool:
        # Implement your complexity detection logic
        complex_keywords = ["explain", "analyze", "compare", "design"]
        return any(kw in query.lower() for kw in complex_keywords)
```
:::
