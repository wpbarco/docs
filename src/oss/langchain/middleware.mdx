---
title: Middleware
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />


Middleware provides a way to more tightly control what happens inside the agent.

The core agent loop involves calling a `model`, letting it choose `tools` to execute, and then finishing when it calls no more tools.


<Card>
```mermaid
%%{
  init: {
    "fontFamily": "monospace",
    "flowchart": {
      "curve": "curve"
    },
    "themeVariables": {"edgeLabelBackground": "transparent"}
  }
}%%
graph TD
  %% Outside the agent
  QUERY([input])
  LLM{model}
  TOOL(tools)
  ANSWER([output])

  %% Main flows (no inline labels)
  QUERY --> LLM
  LLM --"action"--> TOOL
  TOOL --"observation"--> LLM
  LLM --"finish"--> ANSWER

  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;
  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;
  class QUERY blueHighlight;
  class ANSWER blueHighlight;
```
</Card>

Middleware provides control over what happens before and after those steps.

Build custom middleware by implementing any of these hooks on a subclass of the `AgentMiddleware` class:

:::python
| Hook | When it runs | Use cases |
|------|--------------|-----------|
| `before_agent` | Before calling the agent | Load memory, validate input |
| `before_model` | Before each LLM call | Update prompts, trim messages |
| `wrap_model_call` | Around each LLM call | Intercept and modify requests/responses |
| `wrap_tool_call` | Around each tool call | Intercept and modify tool execution |
| `after_model` | After each LLM response | Validate output, apply guardrails |
| `after_agent` | After agent completes | Save results, cleanup |

In addition to that, each middleware can define the following static properties:
- `name`: The name of the middleware (required)
- `tools`: The tools that the middleware makes available to the agent (optional)
- `state_schema`: The schema of the state that the middleware requires (optional)
:::
:::js
| Hook | When it runs | Use cases |
|------|--------------|-----------|
| `beforeAgent` | Before calling the agent | Load memory, validate input |
| `beforeModel` | Before each LLM call | Update prompts, trim messages |
| `wrapModelCall` | Around each LLM call | Intercept and modify requests/responses |
| `wrapToolCall` | Around each tool call | Intercept and modify tool execution |
| `afterModel` | After each LLM response | Validate output, apply guardrails |
| `afterAgent` | After agent completes | Save results, cleanup |

In addition to that, each middleware can define the following static properties:
- `name`: The name of the middleware (required)
- `tools`: The tools that the middleware makes available to the agent (optional)
- `stateSchema`: The schema of the state that the middleware requires (optional)
- `contextSchema`: The schema of the context that the middleware requires (optional)
:::

:::python
An agent can contain any combination of these hooks. Not all hooks need to be implemented.
:::
:::js
An agent can contain multiple middleware. Each middleware does not need to implement all hooks.
:::

<Card>
```mermaid
%%{
  init: {
    "fontFamily": "monospace",
    "flowchart": {
      "curve": "curve"
    },
    "themeVariables": {"edgeLabelBackground": "transparent"}
  }
}%%
graph TD
  %% Outside the agent
  QUERY([input])
  BEFORE_AGENT(Middleware.before_agent)
  BEFORE_MODEL(Middleware.before_model)
  WRAP_MODEL(Middleware.wrap_model_call)
  LLM{model}
  AFTER_MODEL(Middleware.after_model)
  WRAP_TOOL(Middleware.wrap_tool_call)
  TOOL(tools)
  AFTER_AGENT(Middleware.after_agent)
  ANSWER([output])

  %% Main flows (no inline labels)
  QUERY --> BEFORE_AGENT
  BEFORE_AGENT --> BEFORE_MODEL
  BEFORE_MODEL --> WRAP_MODEL
  WRAP_MODEL --> LLM
  LLM --> AFTER_MODEL
  AFTER_MODEL --"action"--> WRAP_TOOL
  WRAP_TOOL --> TOOL
  TOOL --"observation"--> BEFORE_MODEL
  LLM --"finish"--> AFTER_AGENT
  AFTER_AGENT --> ANSWER

  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;
  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;
  class QUERY blueHighlight;
  class ANSWER blueHighlight;
```
</Card>

## Using in an agent

:::python
You can use middleware in an agent by passing it `create_agent`:
```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    ...,
    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],
    ...
)
```
:::
:::js
```typescript
import {
  createAgent,
  summarizationMiddleware,
  humanInTheLoopMiddleware,
} from "langchain";

const agent = createAgent({
  // ...
  middleware: [summarizationMiddleware, humanInTheLoopMiddleware],
  // ...
});
```
:::

Middleware is highly flexible and replaces some other functionality in the agent.
As such, when middleware are used, there are some restrictions on the arguments used to create the agent:
:::python
- `model` must be either a string or a `BaseChatModel`. Will error if a function is passed. If you want to dynamically control the model, use `AgentMiddleware.wrap_model_call`
- `system_prompt` must be either a string or None. Will error if a function is passed. If you want to dynamically control the prompt, use `AgentMiddleware.wrap_model_call`
- `pre_model_hook` must not be provided. Use `AgentMiddleware.before_model` instead.
- `post_model_hook` must not be provided. Use `AgentMiddleware.after_model` instead.
:::
:::js
- `model` must be either a string or a BaseChatModel. Will error if a function is passed. If you want to dynamically control the model, use `AgentMiddleware.wrapModelCall`
- `prompt` must be either a string or None. Will error if a function is passed. If you want to dynamically control the prompt, use `AgentMiddleware.wrapModelCall`
- `preModelHook` must not be provided. Use `AgentMiddleware.beforeModel` instead.
- `postModelHook` must not be provided. Use `AgentMiddleware.afterModel` instead.
:::

## Built-in middleware

LangChain provides several built in middleware to use off-the-shelf

- [Summarization](#summarization)
- [Human-in-the-loop](#human-in-the-loop)
- [Anthropic prompt caching](#anthropic-prompt-caching)

### Summarization

The `summarizationMiddleware` automatically manages conversation history by summarizing older messages when token limits are approached. This middleware monitors the total token count of messages and creates concise summaries to preserve context while staying within model limits.

**Key features:**

- Automatic token counting and threshold monitoring
- Intelligent message partitioning that preserves AI/Tool message pairs
- Customizable summary prompts and token limits

**Use Cases:**

- Long-running conversations that exceed token limits
- Multi-turn dialogues with extensive context

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="openai:gpt-4o-mini",
            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens
            messages_to_keep=20,  # Keep last 20 messages after summary
            summary_prompt="Custom prompt for summarization...",  # Optional
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "openai:gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
      summaryPrompt: "Custom prompt for summarization...", // Optional
    }),
  ],
});
```
:::

**Configuration options:**

:::python
- `model`: Language model to use for generating summaries (required)
- `max_tokens_before_summary`: Token threshold that triggers summarization
- `messages_to_keep`: Number of recent messages to preserve (default: 20)
- `token_counter`: Custom function for counting tokens (defaults to character-based approximation)
- `summary_prompt`: Custom prompt template for summary generation
- `summary_prefix`: Prefix added to system messages containing summaries (default: "## Previous conversation summary:")
:::

:::js

- `model`: Language model to use for generating summaries (required)
- `maxTokensBeforeSummary`: Token threshold that triggers summarization
- `messagesToKeep`: Number of recent messages to preserve (default: 20)
- `tokenCounter`: Custom function for counting tokens (defaults to character-based approximation)
- `summaryPrompt`: Custom prompt template for summary generation
- `summaryPrefix`: Prefix added to system messages containing summaries (default: "## Previous conversation summary:")
:::

The middleware ensures tool call integrity by:

1. Never splitting AI messages from their corresponding tool responses
2. Preserving the most recent messages for continuity
3. Including previous summaries in new summarization cycles

### Human-in-the-loop

The `HumanInTheLoopMiddleware` enables human oversight and intervention for tool calls made by the agents. Please
see [human-in-the-loop documentation](/oss/langchain/human-in-the-loop) for more details.

This middleware intercepts tool executions and allows human operators to approve, modify, reject, or manually respond to tool calls before they execute.

### Anthropic prompt caching

`AnthropicPromptCachingMiddleware` is a middleware that enables you to enable Anthropic's native prompt caching.

Prompt caching enables optimal API usage by allowing resuming from specific prefixes in your prompts.
This is particularly useful for tasks with repetitive prompts or prompts with redundant information.

<Info>
Learn more about Anthropic Prompt Caching (strategies, limitations, etc.) [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).
</Info>

When using prompt caching, you'll likely want to use a checkpointer to store conversation
history across invocations.

:::python
```python
from langchain_anthropic import ChatAnthropic
from langchain.agents.middleware.prompt_caching import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

LONG_PROMPT = """
Please be a helpful assistant.

<Lots more context ...>
"""

agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-latest"),
    system_prompt=LONG_PROMPT,
    middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)

# cache store
agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

# cache hit, system prompt is cached
agent.invoke({"messages": [HumanMessage("What's my name?")]})
```
:::
:::js
```typescript
import { createAgent, HumanMessage, anthropicPromptCachingMiddleware } from "langchain";

const LONG_PROMPT = `
Please be a helpful assistant.

<Lots more context ...>
`;

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-latest",
  prompt: LONG_PROMPT,
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});

// cache store
await agent.invoke({
  messages: [HumanMessage("Hi, my name is Bob")]
});

// cache hit, system prompt is cached
const result = await agent.invoke({
  messages: [HumanMessage("What's my name?")]
});
```
:::

## Custom Middleware

Middleware for agents are subclasses of `AgentMiddleware`, which implement one or more of its hooks.

`AgentMiddleware` provides six different hooks to control the core agent loop:

:::python
- `before_agent`: runs before the agent starts. Can load memory, validate input, or exit early with a jump.
- `before_model`: runs before each model call. Can update prompts, trim messages, or exit early with a jump.
- `wrap_model_call`: wraps each model call. Can intercept and modify the model request/response.
- `wrap_tool_call`: wraps each tool call. Can intercept and modify tool execution.
- `after_model`: runs after each model response. Can validate output, apply guardrails, or exit early with a jump.
- `after_agent`: runs after the agent completes. Can save results or perform cleanup.
:::
:::js
- `beforeAgent`: runs before the agent starts. Can load memory, validate input, or exit early with a jump.
- `beforeModel`: runs before each model call. Can update prompts, trim messages, or exit early with a jump.
- `wrapModelCall`: wraps each model call. Can intercept and modify the model request/response.
- `wrapToolCall`: wraps each tool call. Can intercept and modify tool execution.
- `afterModel`: runs after each model response. Can validate output, apply guardrails, or exit early with a jump.
- `afterAgent`: runs after the agent completes. Can save results or perform cleanup.
:::

In order to **exit early**, you can add a `jump_to` key to the state update with one of the following values:

- `"model"`: Jump to the model node
- `"tools"`: Jump to the tools node
- `"end"`: Jump to the end node

If this is specified, all subsequent middleware will not run.

Learn more about exiting early in the [agent jumps](#agent-jumps) section.

:::python
### `before_agent`
:::
:::js
### `beforeAgent`
:::

Runs before the agent starts. Can load memory, validate input, or exit early.

Signature:
:::python
```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from typing import Any

class MyMiddleware(AgentMiddleware):
    def before_agent(self, state: AgentState) -> dict[str, Any] | None:
        # Validate input before starting
        messages = state.get("messages", [])
        if not messages:
            return {
                "messages": [AIMessage("No input provided")],
                "jump_to": "end"
            }
        return None
```
:::
:::js
```typescript
import { createMiddleware, AIMessage } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  beforeAgent: (state) => {
    if (!state.messages || state.messages.length === 0) {
      return {
        messages: [new AIMessage("No input provided")],
        jumpTo: "end",
      };
    }
    return;
  },
});
```
:::

:::python
### `before_model`
:::
:::js
### `beforeModel`
:::

Runs before each model call. Can update prompts, trim messages, or exit early.

Signature:
:::python
```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain.messages import AIMessage

class MyMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState) -> dict[str, Any] | None:
        # terminate early if the conversation is too long
        if len(state["messages"]) > 50:
            return {
                "messages": [AIMessage("I'm sorry, the conversation has been terminated.")],
                "jump_to": "end"
            }
        return state
```
:::
:::js
```typescript
import { createMiddleware, AIMessage } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  beforeModel: (state) => {
    if (state.messages.length > 50) {
      return {
        messages: [
          new AIMessage("I'm sorry, the conversation has been terminated."),
        ],
        jumpTo: "end",
      };
    }
    return state;
  },
});
```
:::

:::python
### `wrap_model_call`
:::
:::js
### `wrapModelCall`
:::

Wraps each model call, allowing you to intercept and modify the model request and/or response. This is useful for dynamic model selection, modifying system prompts, or intercepting model responses.

Signature:
:::python
```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelRequestHandler
from langchain.messages import AIMessage

class MyMiddleware(AgentMiddleware):
    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:
        # Modify the request before calling the model
        if len(request.state["messages"]) > 10:
            # Use a more advanced model for longer conversations
            modified_request = request.replace(model="gpt-5")
        else:
            modified_request = request.replace(model="gpt-5-nano")

        # Call the model with the modified request
        response = handler(modified_request)

        # Optionally modify the response before returning
        return response
```
:::
:::js
```typescript
import { createMiddleware } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  wrapModelCall: (request, handler) => {
    // Modify the request before calling the model
    let modifiedRequest = request;
    if (request.state.messages.length > 10) {
      modifiedRequest = { ...request, model: "gpt-5" };
    } else {
      modifiedRequest = { ...request, model: "gpt-5-nano" };
    }

    // Call the model with the modified request
    const response = handler(modifiedRequest);

    // Optionally modify the response before returning
    return response;
  },
});
```
:::

:::python
### `wrap_tool_call`
:::
:::js
### `wrapToolCall`
:::

Wraps each tool call, allowing you to intercept and modify tool execution. This is useful for error handling, logging, or modifying tool inputs/outputs.

Signature:
:::python
```python
from langchain.agents.middleware import AgentMiddleware, ToolRequest, ToolRequestHandler
from typing import Any

class MyMiddleware(AgentMiddleware):
    def wrap_tool_call(self, request: ToolRequest, handler: ToolRequestHandler) -> Any:
        # Add logging before tool execution
        print(f"Executing tool: {request.tool_name}")

        try:
            # Call the tool
            result = handler(request)
            return result
        except Exception as e:
            # Handle errors gracefully
            return f"Error executing {request.tool_name}: {str(e)}"
```
:::
:::js
```typescript
import { createMiddleware } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  wrapToolCall: (request, handler) => {
    // Add logging before tool execution
    console.log(`Executing tool: ${request.toolName}`);

    try {
      // Call the tool
      const result = handler(request);
      return result;
    } catch (error) {
      // Handle errors gracefully
      return `Error executing ${request.toolName}: ${error.message}`;
    }
  },
});
```
:::

:::python
### `after_model`
:::
:::js
### `afterModel`
:::

Runs after each model response. Can validate output, apply guardrails, or exit early.

Signature:
:::python
```python
from langchain.agents.middleware import AgentState, AgentUpdate, AgentMiddleware

class MyMiddleware(AgentMiddleware):
    def after_model(self, state: AgentState) -> dict[str, Any] | None:
        # Validate the model's output
        last_message = state["messages"][-1]
        if hasattr(last_message, "content") and "inappropriate" in last_message.content.lower():
            return {
                "messages": [AIMessage("I cannot provide that response.")],
                "jump_to": "end"
            }
        return None
```
:::
:::js
```typescript
import { createMiddleware, AIMessage } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  afterModel: (state) => {
    // Validate the model's output
    const lastMessage = state.messages[state.messages.length - 1];
    if (lastMessage.content && lastMessage.content.toLowerCase().includes("inappropriate")) {
      return {
        messages: [new AIMessage("I cannot provide that response.")],
        jumpTo: "end",
      };
    }
    return;
  },
});
```
:::

:::python
### `after_agent`
:::
:::js
### `afterAgent`
:::

Runs after the agent completes. Can save results or perform cleanup.

Signature:
:::python
```python
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing import Any

class MyMiddleware(AgentMiddleware):
    def after_agent(self, state: AgentState) -> dict[str, Any] | None:
        # Save the conversation to a database
        print(f"Agent completed with {len(state['messages'])} messages")
        # Perform any cleanup or logging
        return None
```
:::
:::js
```typescript
import { createMiddleware } from "langchain";

const myMiddleware = createMiddleware({
  name: "MyMiddleware",
  afterAgent: (state) => {
    // Save the conversation to a database
    console.log(`Agent completed with ${state.messages.length} messages`);
    // Perform any cleanup or logging
    return;
  },
});
```
:::

## New state keys

Middleware can extend the agent's state with custom properties, enabling rich data flow between middleware components and ensuring type safety throughout the agent execution.

### State extension

Middleware can define additional state properties that persist throughout the agent's execution. These properties become part of the agent's state and are available to all hooks for said middleware.

:::python
```python
from langchain.agents.middleware import AgentState, AgentMiddleware

class MyState(AgentState):
    model_call_count: int

class MyMiddleware(AgentMiddleware[MyState]):
    state_schema: MyState

    def before_model(self, state: AgentState) -> dict[str, Any] | None:
        # terminate early if the model has been called too many times
        if state["model_call_count"] > 10:
            return {"jump_to": "end"}
        return state

    def after_model(self, state: AgentState) -> dict[str, Any] | None:
        return {"model_call_count": state["model_call_count"] + 1}
```
:::

:::js
When a middleware defines required state properties through its `stateSchema`, these properties must be provided when invoking the agent:

```typescript
import { createMiddleware, createAgent, HumanMessage } from "langchain";
import { z } from "zod";

// Middleware with custom state requirements
const authMiddleware = createMiddleware({
  name: "AuthMiddleware",
  stateSchema: z.object({
    userId: z.string(), // Required
    userRole: z.string().default("user"), // Optional with default
  }),
  beforeModel: (state) => {
    // Access custom state properties
    console.log(`User ${state.userId} with role ${state.userRole}`);
    return;
  },
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [],
  middleware: [authMiddleware] as const,
});

// TypeScript enforces required state properties
const result = await agent.invoke({
  messages: [new HumanMessage("Hello")],
  userId: "user-123", // Required by middleware
  // userRole is optional due to default value
});
```
:::

### Context extension

<Note>
This is currently only available in JavaScript.
</Note>

Context properties are configuration values passed through the runnable config. Unlike state, context is read-only and typically used for configuration that doesn't change during execution.

:::js
Middleware can define context requirements that must be satisfied through the agent's configuration:

```typescript
import { z } from "zod";
import { createMiddleware, HumanMessage } from "langchain";

const rateLimitMiddleware = createMiddleware({
  name: "RateLimitMiddleware",
  contextSchema: z.object({
    maxRequestsPerMinute: z.number(),
    apiKey: z.string(),
  }),
  beforeModel: async (state, runtime) => {
    // Access context through runtime
    const { maxRequestsPerMinute, apiKey } = runtime.context;

    // Implement rate limiting logic
    const allowed = await checkRateLimit(apiKey, maxRequestsPerMinute);
    if (!allowed) {
      return { jumpTo: "END" };
    }

    return state;
  },
});

// Context is provided through config
await agent.invoke(
  { messages: [new HumanMessage("Process data")] },
  {
    context: {
      maxRequestsPerMinute: 60,
      apiKey: "api-key-123",
    },
  }
);
```
:::

### Combining multiple middleware

When using multiple middleware, their state and context schemas are merged. All required properties from all middleware must be satisfied:

:::python
```python
from langchain.agents.middleware import AgentMiddleware
from langchain.messages import HumanMessage
from typing import Any, Dict

class Middleware1State(AgentState):
    prop_1: str
    shared_prop: int

class Middleware2State(AgentState):
    prop_2: bool
    shared_prop: int

class Middleware1(AgentMiddleware):
    def before_model(self, state: Dict[str, Any]) -> Dict[str, Any] | None:
        # Access prop1 and sharedProp from state
        print(f"Middleware1: prop1={state.get('prop_1')}, sharedProp={state.get('shared_prop')}")
        return None

class Middleware2(AgentMiddleware):
    def before_model(self, state: Dict[str, Any]) -> Dict[str, Any] | None:
        # Access prop2 and sharedProp from state
        print(f"Middleware2: prop2={state.get('prop_2')}, sharedProp={state.get('shared_prop')}")
        return None

agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    middleware=[Middleware1(), Middleware2()],
)
```
:::

:::js
```typescript
import { createMiddleware, createAgent, HumanMessage } from "langchain";
import { z } from "zod";

const middleware1 = createMiddleware({
  name: "Middleware1",
  stateSchema: z.object({
    prop1: z.string(),
    sharedProp: z.number(),
  }),
});

const middleware2 = createMiddleware({
  name: "Middleware2",
  stateSchema: z.object({
    prop2: z.boolean(),
    sharedProp: z.number(), // Same property name must have compatible types
  }),
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [],
  middleware: [middleware1, middleware2] as const,
});

// Must provide all required properties
const result = await agent.invoke({
  messages: [new HumanMessage("Hello")],
  prop1: "value1", // Required by middleware1
  prop2: true, // Required by middleware2
  sharedProp: 42, // Required by both
});
```
:::

### Agent-level context schema

Agents can also define their own context requirements that combine with middleware requirements:

:::python
```python
# ...
```
:::
:::js
```typescript
import { createAgent, HumanMessage } from "langchain";
import { z } from "zod";

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: [],
  contextSchema: z.object({
    environment: z.enum(["development", "production"]),
  }),
  middleware: [rateLimitMiddleware] as const,
});

// Must satisfy both agent and middleware context requirements
await agent.invoke(
  { messages: [new HumanMessage("Deploy application")] },
  {
    context: {
      environment: "production", // Required by agent
      maxRequestsPerMinute: 60, // Required by middleware
      apiKey: "api-key-123", // Required by middleware
    },
  }
);
```
:::

### Best practices

1. **Use State for Dynamic Data**: Properties that change during execution (user session, accumulated data)
2. **Use Context for Configuration**: Static configuration values (API keys, feature flags, limits)
3. **Provide Defaults When Possible**: Use `.default()` in Zod schemas to make properties optional
4. **Document Requirements**: Clearly document what state and context properties your middleware requires
:::js
5. **Type Safety**: Leverage TypeScript's type checking to catch missing properties at compile time

The type system ensures all required properties are provided, preventing runtime errors:

```typescript
// TypeScript error: Property 'userId' is missing
await agent.invoke({
  messages: [new HumanMessage("Hello")],
  // userId is required but not provided
});

// TypeScript error: Type 'number' is not assignable to type 'string'
await agent.invoke({
  messages: [new HumanMessage("Hello")],
  userId: 123, // Wrong type
});
```
:::

## Middleware execution order

You can provide multiple middlewares. They are executed in the following logic:

:::python
**`before_agent`**: Runs once at the start, in the order middleware are passed in. If one exits early, following middleware are not run.
**`before_model`**: Runs before each model call, in the order middleware are passed in. If one exits early, following middleware are not run.
**`wrap_model_call`**: Wraps are executed in the order middleware are passed in, with each wrapper calling the next.
**`wrap_tool_call`**: Wraps are executed in the order middleware are passed in, with each wrapper calling the next.
**`after_model`**: Runs after each model response, in the _reverse_ order that middleware are passed in. If one exits early, following middleware are not run.
**`after_agent`**: Runs once at the end, in the _reverse_ order that middleware are passed in.
:::
:::js
**`beforeAgent`**: Runs once at the start, in the order middleware are passed in. If one exits early, following middleware are not run.
**`beforeModel`**: Runs before each model call, in the order middleware are passed in. If one exits early, following middleware are not run.
**`wrapModelCall`**: Wraps are executed in the order middleware are passed in, with each wrapper calling the next.
**`wrapToolCall`**: Wraps are executed in the order middleware are passed in, with each wrapper calling the next.
**`afterModel`**: Runs after each model response, in the _reverse_ order that middleware are passed in. If one exits early, following middleware are not run.
**`afterAgent`**: Runs once at the end, in the _reverse_ order that middleware are passed in.
:::

## Agent jumps

:::python
In order to **exit early**, you can add a `jump_to` key to the state update with one of the following values:
:::
:::js
In order to **exit early**, you can add a `jumpTo` key to the state update with one of the following values:
:::

- `"model"`: Jump to the model node
- `"tools"`: Jump to the tools node
- `"end"`: Jump to the end node

If this is specified, all subsequent middleware will not run.

:::python
If you jump to `model` node, all `before_model` middleware will run. It's forbidden to jump to `model` from an existing `before_model` middleware.
:::

:::js
If you jump to `model` node, all `beforeModel` middleware will run. It's forbidden to jump to `model` from an existing `beforeModel` middleware.
:::

Example usage:
:::python
```python
from langchain.agents.types import AgentState, AgentUpdate, AgentJump
from langchain.agents.middleware import AgentMiddleware

class MyMiddleware(AgentMiddleware):
    def after_model(self, state: AgentState) -> dict[str, Any]:
        return {
        "messages": ...,
        "jump_to": "model"
    }
```
:::
:::js
```typescript
import { createMiddleware } from "langchain";

const middleware = createMiddleware({
  name: "MyMiddleware",
  afterModel: (state) => {
    // ...
    return {
      messages: [
        /* ... */
      ],
      jumpTo: "model",
    };
  },
});
```
:::

## Examples

### Dynamically selecting tools

In many applications, you may have a large set of tools, but only a small subset is relevant for a specific request. To optimize performance and accuracy, it’s best to **expose only the tools that are needed for each request**.

Doing so provides several benefits:

* **Shorter prompts** – reducing unnecessary complexity.
* **Improved accuracy** – the model chooses from fewer options.
* **Permission control** – can select tools based on user permissions.

Use middleware to dynamically select which tools are available at runtime based on context.

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelRequestHandler
from langchain.messages import AIMessage

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:
        """Middleware to select relevant tools based on state/context."""
        # Select a small, relevant subset of tools based on state/context
        modified_request = request.replace(tools=["relevant_tool_1", "relevant_tool_2"]) # [!code highlight]
        return handler(modified_request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    # Middleware can be used to select a smaller subset that's relevant for the given
    # run.
    middleware=[ToolSelectorMiddleware()], # [!code highlight]
)
```
:::
:::js
```typescript
import { createAgent, createMiddleware } from "langchain";

const toolSelectorMiddleware = createMiddleware({
  name: "ToolSelector",
  wrapModelCall: (request, handler) => {
    // Select a small, relevant subset of tools based on state/context
    const modifiedRequest = { ...request, tools: ["relevant_tool_1", "relevant_tool_2"] }; // [!code highlight]
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: allTools,  // All available tools need to be registered upfront
  // Middleware can be used to select a smaller subset that's relevant for the given run.
  middleware: [toolSelectorMiddleware], // [!code highlight]
});
```
:::


<Expandable title="Extended example: Select tools based on runtime context">

This example shows how to select between GitHub and GitLab tools based on the user's provider.

:::python
```python Expandable
from dataclasses import dataclass
from typing import Literal

from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelRequestHandler
from langchain.tools import tool
from langchain.messages import AIMessage

@tool
def github_create_issue(repo: str, title: str) -> dict:
    """Create an issue in a GitHub repository."""
    return {"url": f"https://github.com/{repo}/issues/1", "title": title}

@tool
def gitlab_create_issue(project: str, title: str) -> dict:
    """Create an issue in a GitLab project."""
    return {"url": f"https://gitlab.com/{project}/-/issues/1", "title": title}

all_tools = [github_create_issue, gitlab_create_issue]

@dataclass
class Context:
    provider: Literal["github", "gitlab"]

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:
        """Select tools based on the VCS provider."""
        provider = request.runtime.context.provider
        selected_tools = ["gitlab_create_issue"] if provider == "gitlab" else ["github_create_issue"]
        modified_request = request.replace(tools=selected_tools)
        return handler(modified_request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,
    middleware=[ToolSelectorMiddleware()],
    context_schema=Context,
)

# Invoke with GitHub context
agent.invoke(
    {
        "messages": [{
            "role": "user",
            "content": "Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"
        }]
    },
    context=Context(provider="github"),
)
```
:::

:::js
```typescript Expandable
import { z } from "zod";
import { createAgent, createMiddleware, tool, HumanMessage } from "langchain";

const githubCreateIssue = tool(
  async ({ repo, title }) => ({
    url: `https://github.com/${repo}/issues/1`,
    title,
  }),
  {
    name: "github_create_issue",
    description: "Create an issue in a GitHub repository",
    schema: z.object({ repo: z.string(), title: z.string() }),
  }
);

const gitlabCreateIssue = tool(
  async ({ project, title }) => ({
    url: `https://gitlab.com/${project}/-/issues/1`,
    title,
  }),
  {
    name: "gitlab_create_issue",
    description: "Create an issue in a GitLab project",
    schema: z.object({ project: z.string(), title: z.string() }),
  }
);

const allTools = [githubCreateIssue, gitlabCreateIssue];

const toolSelector = createMiddleware({
  name: "toolSelector",
  contextSchema: z.object({ provider: z.enum(["github", "gitlab"]) }),
  wrapModelCall: (request, handler) => {
    const provider = request.runtime.context.provider;
    const toolName = provider === "gitlab" ? "gitlab_create_issue" : "github_create_issue";
    const modifiedRequest = { ...request, tools: [toolName] };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "openai:gpt-4o",
  tools: allTools,
  middleware: [toolSelector],
});

// Invoke with GitHub context
await agent.invoke(
  {
    messages: [
      new HumanMessage("Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"),
    ],
  },
  {
    context: { provider: "github" },
  }
);
```
:::

**Key points:**

- Register all tools with the agent upfront
- Use middleware to select the relevant subset per request
- Define required context properties using `contextSchema`
- Use context for configuration that doesn't change during execution
- Use state for values that change during the agent run

</Expandable>
