---
title: Quickstart
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

Welcome to LangChain! This quickstart will take you from zero to a fully functional AI agent in just a few minutes. We'll start simple and gradually build up to something more sophisticated.

## ðŸš€ Super Quick Start

Let's begin with the absolute basics - creating a simple agent that can answer questions and use tools. This minimal example shows the core concept:

:::python
```python
from langchain.agents import create_react_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```
:::

:::js
```ts
import { createReactAgent, tool } from "langchain";

const getWeather = tool((city: string) => `It's always sunny in ${city}!`, {
  name: "get_weather",
  description: "Get the weather for a given city",
});

const agent = createReactAgent({
  model: "anthropic:claude-3-7-sonnet-latest",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);
```
:::

**What just happened?** We created an agent with:
- A language model (Claude 3.7 Sonnet)
- A simple tool (weather function)
- A basic prompt
- The ability to invoke it with messages

## ðŸ—ï¸ Building a Real-World Agent

Now let's create something more practical! We'll build a weather forecasting agent that demonstrates the key concepts you'll use in production:

1. **Detailed system prompts** for better agent behavior
2. **Real-world tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions

Let's walk through each step:

### Step 1: Define the System Prompt

The system prompt is your agent's personality and instructions. Make it specific and actionable:

:::python
```python
# Step 1: define system prompt

system_prompt = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean whereever they are, use the get_user_location tool to find their location."""
```
:::

:::js
```ts
// Step 1: define system prompt
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean whereever they are, use the get_user_location tool to find their location.`;
```
:::

### Step 2: Create Your Tools



:::python
Tools are functions your agent can call. They should be well-documented.
Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so.
Notice here how the `get_user_info` tool does exactly that.

```python
# Step 2: define tools
from langchain_core.tools import tool

def get_weather(city: str) -> str:  # (1)!
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

from langchain_core.runnables import RunnableConfig

USER_LOCATION = {
    "1": "Florida",
    "2": "SF"
}

@tool
def get_user_info(config: RunnableConfig) -> str:
    """Retrieve user information based on user ID."""
    user_id = config["context"].get("user_id")
    return USER_LOCATION[user_id]
```
:::

:::js
Tools are functions your agent can call. They should be well-documented.
Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so.
Notice here how the `getUserInfo` tool does exactly that.
```ts
import { tool } from "langchain";
import z from "zod";

// Step 2: define tools
const getWeather = tool(({ city }) => `It's always sunny in ${city}!`, {
  name: "get_weather",
  description: "Get the weather for a given city",
  schema: z.object({
    city: z.string(),
  }),
});

const USER_LOCATION = {
  "1": "Florida",
  "2": "SF",
} as const;

const getUserInfo = tool(
  (_, config) => {
    const { user_id } = config.context as {
      user_id: keyof typeof USER_LOCATION;
    };
    console.log("user_id", config.context);
    return USER_LOCATION[user_id];
  },
  {
    name: "get_user_info",
    description: "Retrieve user information based on user ID",
    schema: z.object({}),
  }
);
```
:::

### Step 3: Configure Your Model

Set up your language model with the right parameters for your use case:

:::python
```python
# Step 3: define model

from langchain.chat_models import init_chat_model

model = init_chat_model(
    "anthropic:claude-3-7-sonnet-latest",
    temperature=0
)
```
:::

:::js
```ts
// Step 3: define model
import { initChatModel } from "langchain/chat_models";

const model = initChatModel(
  "anthropic:claude-3-7-sonnet-latest", 
  temperature: 0
);
```
:::

### Step 4: Define Response Format

Structured outputs ensure your agent returns data in a predictable format:

:::python
```python
# Step 4: define response format

from dataclasses import dataclass

@dataclass
class WeatherResponse:
    conditions: str
    punny_response: str
```
:::

:::js
```ts
// Step 3: define response format
const responseFormat = z.object({
  conditions: z.string(),
  punny_response: z.string(),
});
```
:::

### Step 5: Add Memory

Enable your agent to remember conversation history:

:::python
```python
# Step 5: define checkpointer
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
```
:::

:::js
```ts
import { MemorySaver } from "langchain";

// Step 4: define checkpointer
const checkpointer = new MemorySaver();
```
:::

### Step 6: Bring It All Together

Now assemble your agent with all the components:

:::python
```python
# Step 6: create agent
agent = create_agent(
    model=model,
    prompt=system_prompt,
    tools=[get_user_info, get_weather],
    response_format=WeatherResponse,
    checkpointer=checkpointer
)

config = {"configurable": {"thread_id": "1"}}
context = {"user_id": "1"}
response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
    config=config,
    context=context
)

response['structured_response']

response = agent.invoke(
    {"messages": [{"role": "user", "content": "thank you!"}]},
    config=config,
    context=context
)

response['structured_response']
```
:::

:::js
```ts
import { createReactAgent } from "langchain";

// Step 5: create agent
const agent = createReactAgent({
  model: "anthropic:claude-3-7-sonnet-latest",
  prompt: systemPrompt,
  tools: [getUserInfo, getWeather],
  responseFormat,
  checkpointer,
});

const config = {
  configurable: { thread_id: "1" },
  context: { user_id: "1" },
};
const response = await agent.invoke(
  { messages: [{ role: "user", content: "what is the weather outside?" }] },
  config
);
console.log(response.structuredResponse);

const thankYouResponse = await agent.invoke(
  { messages: [{ role: "user", content: "thank you!" }] },
  config
);
console.log(thankYouResponse.structuredResponse);
```
:::

## ðŸŽ¯ What You've Built

Congratulations! You now have a sophisticated AI agent that can:
- **Understand context** and remember conversations
- **Use multiple tools** intelligently
- **Provide structured responses** in a consistent format
- **Handle user-specific information** through context
- **Maintain conversation state** across interactions
