# Response metadata

Many model providers include some metadata in their chat generation [responses](/oss/concepts/messages/#aimessage). This metadata can be accessed via the `AIMessage.response_metadata: Dict` attribute. Depending on the model provider and model configuration, this can contain information like [token counts](/oss/how-to/chat_token_usage_tracking), [logprobs](/oss/how-to/logprobs), and more.

Here's what the response metadata looks like for a few different providers:

## OpenAI


```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'token_usage': {'completion_tokens': 88,
  'prompt_tokens': 16,
  'total_tokens': 104,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-mini-2024-07-18',
 'system_fingerprint': 'fp_34a54ae93c',
 'id': 'chatcmpl-ByN1Qkvqb5fAGKKzXXxZ3rBlnqkWs',
 'service_tier': 'default',
 'finish_reason': 'stop',
 'logprobs': None}
```


## Anthropic


```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-7-sonnet-20250219")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'id': 'msg_01NTWnqvbNKSjGfqQL7xikau',
 'model': 'claude-3-7-sonnet-20250219',
 'stop_reason': 'end_turn',
 'stop_sequence': None,
 'usage': {'cache_creation_input_tokens': 0,
  'cache_read_input_tokens': 0,
  'input_tokens': 17,
  'output_tokens': 197,
  'server_tool_use': None,
  'service_tier': 'standard'},
 'model_name': 'claude-3-7-sonnet-20250219'}
```


## Google Generative AI


```python
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},
 'finish_reason': 'STOP',
 'model_name': 'gemini-2.5-flash',
 'safety_ratings': []}
```


## Bedrock (Anthropic)


```python
from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(model="anthropic.claude-3-7-sonnet-20250219-v1:0")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'ResponseMetadata': {'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'date': 'Sat, 22 Mar 2025 11:27:46 GMT',
   'content-type': 'application/json',
   'content-length': '1660',
   'connection': 'keep-alive',
   'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'},
  'RetryAttempts': 0},
 'stopReason': 'end_turn',
 'metrics': {'latencyMs': [11044]}}
```


## MistralAI


```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(model="mistral-small-latest")
msg = llm.invoke([("human", "What's the oldest known example of cuneiform")])
msg.response_metadata
```



```output
{'token_usage': {'prompt_tokens': 13,
  'total_tokens': 306,
  'completion_tokens': 293},
 'model_name': 'mistral-small-latest',
 'model': 'mistral-small-latest',
 'finish_reason': 'stop'}
```


## Groq


```python
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'token_usage': {'completion_tokens': 184,
  'prompt_tokens': 45,
  'total_tokens': 229,
  'completion_time': 0.245333333,
  'prompt_time': 0.002262803,
  'queue_time': 0.19315161,
  'total_time': 0.247596136},
 'model_name': 'llama-3.1-8b-instant',
 'system_fingerprint': 'fp_a56f6eea01',
 'finish_reason': 'stop',
 'logprobs': None}
```


## FireworksAI


```python
from langchain_fireworks import ChatFireworks

llm = ChatFireworks(model="accounts/fireworks/models/llama-v3p1-70b-instruct")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```



```output
{'token_usage': {'prompt_tokens': 25,
  'total_tokens': 352,
  'completion_tokens': 327},
 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct',
 'system_fingerprint': '',
 'finish_reason': 'stop',
 'logprobs': None}
```
