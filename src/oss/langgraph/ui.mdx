---
title: Agent Chat UI
---

import StableCalloutPy from '/snippets/stable-lg-callout-py.mdx';
import StableCalloutJS from '/snippets/stable-lg-callout-js.mdx';
import chat_uiPy from '/snippets/oss/ui-py.mdx';
import chat_uiJS from '/snippets/oss/ui-js.mdx';

:::python
<StableCalloutPy />
:::
:::js
<StableCalloutJS />
:::

:::python
<chat_uiPy />
:::
:::js
<chat_uiJS />
:::

### Connect to your agent

Agent Chat UI can connect to both [local](/oss/langgraph/studio#setup-local-langgraph-server) and [deployed agents](/oss/langgraph/deploy).

After starting Agent Chat UI, you'll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your LangGraph server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local LangGraph server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

<Tip>
  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).
</Tip>
