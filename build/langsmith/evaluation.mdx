---
title: LangSmith Evaluations
sidebarTitle: Overview
mode: wide
---

The following sections help you create datasets, run evaluations, and analyze results:

<Columns cols={3}>

  <Card
    title="Evaluation concepts"
    icon="circle-info"
    href="/langsmith/evaluation-concepts"
    arrow="true"
  >
    Review core terminology and concepts to understand how evaluations work in LangSmith.
  </Card>

  <Card
    title="Manage datasets"
    icon="database"
    href="/langsmith/manage-datasets"
    arrow="true"
  >
    Create and manage datasets for evaluation through the UI or SDK.
  </Card>

  <Card
    title="Run evaluations"
    icon="microscope"
    href="/langsmith/evaluate-llm-application"
    arrow="true"
  >
    Evaluate your applications with different evaluators and techniques to measure quality.
  </Card>

  <Card
    title="Analyze results"
    icon="chart-bar"
    href="/langsmith/analyze-an-experiment"
    arrow="true"
  >
    View and analyze evaluation results, compare experiments, filter data, and export findings.
  </Card>

  <Card
    title="Collect feedback"
    icon="comments"
    href="/langsmith/annotation-queues"
    arrow="true"
  >
    Gather human feedback through annotation queues and inline annotation on outputs.
  </Card>

  <Card
    title="Follow tutorials"
    icon="book"
    href="/langsmith/evaluate-chatbot-tutorial"
    arrow="true"
  >
    Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.
  </Card>

</Columns>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
