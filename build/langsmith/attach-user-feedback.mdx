---
title: Log user feedback using the SDK
sidebarTitle: Log user feedback using the SDK
---

<Tip>
**Key concepts**
- [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
- [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

LangSmith makes it easy to attach feedback to traces.
This feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.

## Use [create_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Here we'll walk through how to log feedback using the SDK.

<Info>
**Child runs**
You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.
This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.
</Info>

<Tip>
**Non-blocking creation (Python only)**
The Python client will automatically background feedback creation if you pass `trace_id=` to [create_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).
This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.
</Tip>

<CodeGroup>

```python Python
from langsmith import trace, traceable, Client

    @traceable
    def foo(x):
        return {"y": x * 2}

    @traceable
    def bar(y):
        return {"z": y - 1}

    client = Client()

    inputs = {"x": 1}
    with trace(name="foobar", inputs=inputs) as root_run:
        result = foo(**inputs)
        result = bar(**result)
        root_run.outputs = result
        trace_id = root_run.id
        child_runs = root_run.child_runs

    # Provide feedback for a trace (a.k.a. a root run)
    client.create_feedback(
        key="user_feedback",
        score=1,
        trace_id=trace_id,
        comment="the user said that ..."
    )

# Provide feedback for a child run
foo_run_id = [run for run in child_runs if run.name == "foo"][0].id
client.create_feedback(
    key="correctness",
    score=0,
    run_id=foo_run_id,
    # trace_id= is optional but recommended to enable batched and backgrounded
    # feedback ingestion.
    trace_id=trace_id,
)
```

```typescript TypeScript
import { Client } from "langsmith";
const client = new Client();

    // ... Run your application and get the run_id...
    // This information can be the result of a user-facing feedback form

await client.createFeedback(
    runId,
    "feedback-key",
    {
        score: 1.0,
        comment: "comment",
    }
);
```

</CodeGroup>

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](/langsmith/access-current-span) for how to get the run ID of an in-progress run.

To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](/langsmith/filter-traces-in-application).

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/attach-user-feedback.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
