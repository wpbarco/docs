---
title: ChatDeepInfra
---

LangChain supports chat models hosted by [Deep Infra](https://deepinfra.com/) through the `ChatDeepInfra` wrapper.
First, you'll need to install the `@langchain/community` package:

<Tip>
See [this section for general instructions on installing LangChain packages](/oss/javascript/langchain/install).
</Tip>

```bash npm
npm install @langchain/community @langchain/core
```

You'll need to obtain an API key and set it as an environment variable named `DEEPINFRA_API_TOKEN`
(or pass it into the constructor), then call the model as shown below:

```typescript
import { ChatDeepInfra } from "@langchain/community/chat_models/deepinfra";
import { HumanMessage } from "@langchain/core/messages";

const apiKey = process.env.DEEPINFRA_API_TOKEN;

const model = "meta-llama/Meta-Llama-3-70B-Instruct";

const chat = new ChatDeepInfra({
  model,
  apiKey,
});

const messages = [new HumanMessage("Hello")];

const res = await chat.invoke(messages);

console.log(res);
```

## Related

- Chat model [conceptual guide](/oss/javascript/langchain/models)
- Chat model [how-to guides](/oss/javascript/langchain/models)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/deep_infra.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    Connect these docs to Claude, VSCode, and more via MCP for    real-time answers. [See how](/use-these-docs)
</Tip>
