---
title: Model caches
---

[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

<Columns cols={3}>
  <Card
    title="Azure Cosmos DB NoSQL Semantic Cache"
    icon="link"
    href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql"
    arrow="true"
    cta="View guide"
  >
  </Card>
</Columns>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
