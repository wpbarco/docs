---
title: AimlapiLLM
---

<Warning>
**You are currently on a page documenting the use of AI/ML API models as text completion models. Many of the latest and most popular AI/ML API models are [chat completion models](/oss/python/langchain/models).**

You may be looking for [this page instead](/oss/python/integrations/chat/aimlapi).
</Warning>

This page helps you get started with AI/ML API text completion models. For detailed documentation of all `AimlapiLLM` features and configurations, head to the [API reference](https://python.langchain.com/api_reference/aimlapi/llms/langchain_aimlapi.llms.AimlapiLLM.html).

## Overview

### Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: |
| [AimlapiLLM](https://python.langchain.com/api_reference/aimlapi/llms/langchain_aimlapi.llms.AimlapiLLM.html) | [langchain-aimlapi](https://python.langchain.com/api_reference/aimlapi/index.html) | ❌ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-aimlapi?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-aimlapi?style=flat-square&label=%20) |

### Model features

| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |

## Setup

To access AI/ML API models you'll need to create an account, get an API key, and install the `langchain-aimlapi` integration package.

### Credentials

Head to [aimlapi.com](https://aimlapi.com/app/?utm_source=langchain&utm_medium=github&utm_campaign=integration) to sign up and generate an API key. Once you've done this set the `AIMLAPI_API_KEY` environment variable:

```python
import getpass
import os

if not os.getenv("AIMLAPI_API_KEY"):
    os.environ["AIMLAPI_API_KEY"] = getpass.getpass("Enter your AI/ML API key: ")
```

To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

### Installation

The LangChain AI/ML API integration lives in the `langchain-aimlapi` package:

```python
pip install -qU langchain-aimlapi
```

## Instantiation

Now we can instantiate our model object and generate text completions:

```python
from langchain_aimlapi import AimlapiLLM

llm = AimlapiLLM(
    model="gpt-3.5-turbo-instruct",
    temperature=0.5,
    max_tokens=256,
)
```

## Invocation

```python
response = llm.invoke("Explain the bubble sort algorithm in Python.")
print(response)
```

```output
Bubble sort is a simple sorting algorithm that repeatedly steps through a list, compares adjacent items, and swaps them when they are out of order. The process repeats until the entire list is sorted. While easy to understand and implement, bubble sort is inefficient on large datasets because it has quadratic time complexity.
```

## Streaming invocation

You can also stream responses token-by-token:

```python
llm = AimlapiLLM(
    model="gpt-3.5-turbo-instruct",
)

for chunk in llm.stream("List top 5 programming languages in 2025 with reasons."):
    print(chunk, end="", flush=True)
```

## API reference

For detailed documentation of all AimlapiLLM features and configurations head to the [API reference](https://python.langchain.com/api_reference/aimlapi/llms/langchain_aimlapi.llms.AimlapiLLM.html).

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/llms/aimlapi.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>
